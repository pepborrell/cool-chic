preset_name: c3x
warmup:
  phases:
    - candidates: 5
      training_phase:
        lr: 1e-2
        max_itr: 400
        freq_valid: 400
        patience: 100000
        quantize_model: False
        schedule_lr: False
        softround_temperature: [0.3, 0.3]
        noise_parameter: [2.0, 2.0]
        quantizer_noise_type: "kumaraswamy"
        quantizer_type: "softround"
        optimized_module: ["all"]
    - candidates: 2
      training_phase:
        lr: 1e-2
        max_itr: 400
        freq_valid: 400
        patience: 100000
        quantize_model: False
        schedule_lr: False
        softround_temperature: [0.3, 0.3]
        noise_parameter: [2.0, 2.0]
        quantizer_noise_type: "kumaraswamy"
        quantizer_type: "softround"
        optimized_module: ["all"]
all_phases:
  # 1st stage: with soft round and quantization noise
  - lr: 1e-2
    max_itr: 10600 # 10000 + 600
    patience: 5000
    optimized_module: ["all"]
    schedule_lr: True
    quantizer_type: softround
    quantizer_noise_type: gaussian
    softround_temperature: [0.3, 0.1]
    noise_parameter: [ 0.25, 0.1 ]
  # Stage with STE then network quantization
  - lr: 1e-4
    max_itr: 1500
    patience: 1500
    optimized_module: ["all"]
    schedule_lr: True
    quantizer_type: ste
    quantizer_noise_type: none
    # This is only used to parameterize the backward of the quantization
    softround_temperature: [ 1e-4, 1e-4 ]
    noise_parameter: [ 1.0, 1.0 ]  # not used since quantizer type is "ste"
    quantize_model: True  # ! This is an important parameter
  # Re-tune the latent
  - lr: 1.0e-4
    max_itr: 1000
    patience: 50
    quantizer_type: ste
    quantizer_noise_type: none
    optimized_module: ["latent"]  # ! Only fine tune the latent
    freq_valid: 10
    softround_temperature: [ 1e-4, 1e-4 ]
    noise_parameter: [ 1.0, 1.0 ]  # not used since quantizer type is "ste"
